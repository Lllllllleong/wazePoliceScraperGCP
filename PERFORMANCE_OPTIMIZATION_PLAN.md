# Performance Optimization Plan

This document outlines the steps to implement GZIP compression and optimize Firestore data access for the Waze Police Scraper backend.

## Executive Summary
**Impact Analysis:**
- **GZIP Compression**: High impact - affects all queries, reduces bandwidth by 70-90%
- **Firestore Streaming**: Low-medium impact - only benefits non-archived queries (typically recent dates)

## Phase 0: Pre-Implementation Analysis

### Current Architecture Review
‚úÖ **GCS Archives**: Stored as `.jsonl` (uncompressed) - confirmed from `cmd/alerts-service/main.go:270`
‚úÖ **Frontend Usage**: Confirmed fields via `dataAnalysis/public/app.js:307-366`
  - **Used**: `UUID`, `Type`, `Subtype`, `Street`, `City`, `Country`, `LocationGeo`, `Reliability`, `Confidence`, `PublishTime`, `ExpireTime`, `ScrapeTime`, `ActiveMillis`, `LastVerificationMillis`, `NThumbsUpLast`, `ReportRating`
  - **Unused**: `RawDataInitial`, `RawDataLast` ‚úÖ
‚úÖ **Current Imports**: `iterator` package NOT imported, needs to be added

### Critical Findings
‚ö†Ô∏è **GCS archives INCLUDE raw data** - The archive service queries Firestore using `GetPoliceAlertsByDateRange`, which returns full `PoliceAlert` objects including `RawDataInitial` and `RawDataLast`. These large fields are then written to JSONL archives and transmitted to clients, unnecessarily inflating archive file sizes.
‚ö†Ô∏è **Archive service also needs optimization** - `cmd/archive-service/main.go` uses `GetPoliceAlertsByDateRange` and should also implement field projection to exclude raw data fields when creating archives.
‚ö†Ô∏è **Both services benefit from GZIP** - GZIP compression will help both the alerts-service (serving archives) and archive-service (uploading to GCS).

## Phase 1: Enable GZIP Compression ‚≠ê HIGH PRIORITY
**Goal:** Reduce payload size by 70-90% for text-based JSON responses, significantly improving download speeds for clients.

**Impact:** Affects ALL queries (both GCS and Firestore paths) and ALL services (alerts-service responses + archive-service GCS uploads), immediate user experience improvement and reduced storage costs.

### Implementation Steps

#### 1.1 Add Required Import
Add `"compress/gzip"` to the import block in `cmd/alerts-service/main.go`.

#### 1.2 Implement GZIP Middleware
**Modify `cmd/alerts-service/main.go`:**

Define a `gzipResponseWriter` struct that properly implements the required interfaces:
```go
type gzipResponseWriter struct {
    *gzip.Writer
    http.ResponseWriter
}

func (w *gzipResponseWriter) Write(b []byte) (int, error) {
    return w.Writer.Write(b)
}

func (w *gzipResponseWriter) Flush() {
    w.Writer.Flush()
    if f, ok := w.ResponseWriter.(http.Flusher); ok {
        f.Flush()
    }
}
```

**Note:** Frequent flushing can degrade compression. Consider buffering or reducing flush frequency in `alertsHandler` if compression ratio is poor.

Create the `gzipMiddleware` function:
```go
func gzipMiddleware(next http.HandlerFunc) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        if !strings.Contains(r.Header.Get("Accept-Encoding"), "gzip") {
            next(w, r)
            return
        }
        w.Header().Set("Content-Encoding", "gzip")
        w.Header().Set("Vary", "Accept-Encoding") // IMPORTANT: Cache compatibility
        gz := gzip.NewWriter(w)
        defer gz.Close()
        gzw := &gzipResponseWriter{Writer: gz, ResponseWriter: w}
        next(gzw, r)
    }
}
```

#### 1.3 Apply Middleware
Update the route registration in `main()`:
```go
http.HandleFunc("/police_alerts", corsMiddleware(s.rateLimitMiddleware(gzipMiddleware(s.alertsHandler))))
```

**Note:** Added `Vary: Accept-Encoding` header for proper CDN/cache behavior.

## Phase 2: Firestore Optimization (Streaming) 
**Goal:** Reduce server memory usage (OOM prevention) and latency by streaming results instead of buffering.

**Impact Assessment:**
- ‚úÖ **Streaming**: Prevents OOM for large Firestore queries (good for defensive programming)
- üìä **Scope**: Affects ~5% of alerts-service queries + 100% of archive-service operations



### Implementation Steps

#### 2.1 Add Required Import
#### 2.2 Update Models (CRITICAL DECISION)
**Modify `internal/models/alert.go`:**

**DECISION: Do NOT add `json:"-"` tags to raw data fields.** 
We will retain `RawDataInitial` and `RawDataLast` in the archives for data lineage and debugging purposes. We will rely on GZIP (Phase 1) to handle the file size reduction.

#### 2.3 Create Iterator Method
**Modify `internal/storage/police_alerts.go`:**

Create a new method `GetPoliceAlertsIterator(ctx, startDate, endDate)`.
**Note:** Ensure `RawDataInitial` and `RawDataLast` are included in the `.Select()` list (or omit `.Select()` to fetch all) so that archives generated by `archive-service` remain complete. For the `alerts-service` fallback, we can optionally use a restricted `.Select()` to save bandwidth, but given the low volume of fallback queries, fetching full documents is acceptable.

```go
func (fc *FirestoreClient) GetPoliceAlertsIterator(ctx context.Context, startDate, endDate time.Time) *firestore.DocumentIterator {
    log.Printf("Streaming police alerts from %s to %s", startDate.Format("2006-01-02"), endDate.Format("2006-01-02"))

    // Fetch all fields to ensure archives are complete. 
    // Bandwidth optimization is handled by GZIP.
    return fc.client.Collection(fc.collectionName).
        Where("expire_time", ">=", startDate).
        Where("publish_time", "<=", endDate).
        OrderBy("expire_time", firestore.Asc).
        OrderBy("publish_time", firestore.Asc).
        Documents(ctx)
}
```     Documents(ctx)
}
```

#### 2.4 Update Alerts Handler
**Modify `cmd/alerts-service/main.go`:**

Replace the Firestore fallback code (currently at line ~308-320) with streaming implementation:

```go
} else if err == gcs.ErrObjectNotExist {
    // Archive does not exist, query Firestore with streaming
    startOfDay := time.Date(date.Year(), date.Month(), date.Day(), 0, 0, 0, 0, loc)
    endOfDay := startOfDay.Add(24*time.Hour - time.Second)

    iter := s.firestoreClient.GetPoliceAlertsIterator(ctx, startOfDay, endOfDay)
    defer iter.Stop()

    alertCount := 0
    for {
        doc, err := iter.Next()
        if err == iterator.Done {
            break
        }
        if err != nil {
            log.Printf("Error iterating Firestore for %s: %v", date.Format("2006-01-02"), err)
            break
        }

        var alert models.PoliceAlert
        if err := doc.DataTo(&alert); err != nil {
            log.Printf("Error parsing alert %s: %v", doc.Ref.ID, err)
            continue
        }

        jsonData, marshalErr := json.Marshal(alert)
        if marshalErr != nil {
            log.Printf("Error marshaling alert %s: %v", alert.UUID, marshalErr)
            continue
        }
        dataChan <- append(jsonData, '\n')
        alertCount++
    }
    log.Printf("Streamed %d alerts for %s from Firestore", alertCount, date.Format("2006-01-02"))
}
```

**Improvements from Original:**
- ‚úÖ Added `alertCount` logging for monitoring
- ‚úÖ Better error message uses `doc.Ref.ID` when UUID unavailable
- ‚úÖ Log summary after completion

#### 2.5 Update Archive Service (IMPORTANT)
**Modify `cmd/archive-service/main.go`:**

The archive service currently uses `GetPoliceAlertsByDateRange`, which returns full objects including raw data. This should also be updated to use the iterator method:

```go
// Replace the existing query code with:
iter := firestoreClient.GetPoliceAlertsIterator(ctx, startDate, endDate)
defer iter.Stop()

var alerts []models.PoliceAlert
for {
    doc, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Printf("Error iterating Firestore: %v", err)
        return err
    }

    var alert models.PoliceAlert
    if err := doc.DataTo(&alert); err != nil {
        log.Printf("Error parsing alert %s: %v", doc.Ref.ID, err)
        continue
    }
    alerts = append(alerts, alert)
}
```

**Impact:**
- Improves consistency between live queries and archived data
- Prevents OOM for large archive jobs

## Phase 3: Testing & Verification

### 3.1 Local Development Testing

#### Test 1: GZIP Compression (Critical)
```bash
# Start service locally
go run cmd/alerts-service/main.go

# Test compressed response (Windows/Git Bash)
curl -H "Accept-Encoding: gzip" \
     "http://localhost:8080/police_alerts?dates=2025-10-15" \
     --compressed -v 2>&1 | grep -i "content-encoding"

# Alternative for Windows PowerShell:
# Invoke-WebRequest -Uri "http://localhost:8080/police_alerts?dates=2025-10-15" -Headers @{"Accept-Encoding"="gzip"} -Verbose

# Expected: Content-Encoding: gzip
# Expected: Vary: Accept-Encoding

# Test uncompressed fallback
curl "http://localhost:8080/police_alerts?dates=2025-10-15" -v 2>&1 | grep -i "content-encoding"

# Expected: No Content-Encoding header
```

**Validation Checklist:**
- [ ] Response includes `Content-Encoding: gzip` header when requested
- [ ] Response includes `Vary: Accept-Encoding` header (cache compatibility)
- [ ] Data decompresses correctly (use `--compressed` flag)
- [ ] Uncompressed requests work without compression
- [ ] No broken streaming (data should flush progressively)

#### Test 2: Compression Ratio Measurement
```bash
# Compare sizes
curl -H "Accept-Encoding: gzip" "http://localhost:8080/police_alerts?dates=2025-10-15" \
     --compressed -w '\nDownloaded: %{size_download} bytes\n' -o /dev/null

curl "http://localhost:8080/police_alerts?dates=2025-10-15" \
     -w '\nDownloaded: %{size_download} bytes\n' -o /dev/null

# Calculate ratio: (uncompressed - compressed) / uncompressed * 100
```

**Expected Results:**
- Compression ratio: 70-90% reduction
- Example: 20MB ‚Üí 2-6MB

#### Test 3: Firestore Streaming (If Phase 2 Implemented)
```bash
# Test alerts-service with a date that has NO archive (requires Firestore query)
# Use a recent date or manually delete an archive file for testing
curl "http://localhost:8080/police_alerts?dates=2025-11-30" -v

# Check logs for:
# - "Streaming police alerts from..."
# - "Streamed X alerts for 2025-11-30 from Firestore"
```

**Validation Checklist:**
- [ ] Logs show "Streaming police alerts..." message
- [ ] Response returns valid JSONL data
- [ ] `LastVerificationMillis` field is present (if applicable)
- [ ] No memory spikes in `top` or `htop` during query

#### Test 3b: Archive Service Optimization
```bash
# Test archive-service locally (if updated with Phase 2)
go run cmd/archive-service/main.go

# After it creates a new archive, download and inspect it
gsutil cp gs://YOUR_BUCKET/2025-11-30.jsonl ./test-archive.jsonl

# Verify the archive does NOT contain raw data fields
cat test-archive.jsonl | head -1 | jq 'keys'
# Expected: Should NOT see "RawDataInitial" or "RawDataLast" in the keys

# Compare file sizes (if you have an old archive to compare)
ls -lh ./test-archive.jsonl
# Expected: ~30-50% smaller than old archives with raw data
```

**Validation Checklist:**
- [ ] New archives exclude `RawDataInitial` and `RawDataLast`
- [ ] Archive file size is significantly smaller than previous archives
- [ ] All required frontend fields are present in archive
- [ ] Archive upload to GCS completes faster
#### Test 4: Concurrent Request Handling
```bash
# Simulate 7 concurrent requests (worker pool limit)
# Note: Bash loop syntax for Windows Git Bash
for i in $(seq 1 7); do
    curl -H "Accept-Encoding: gzip" \
         "http://localhost:8080/police_alerts?dates=2025-10-$((10+i))" \
         --compressed -o "output_${i}.jsonl" &
done
wait

# Verify all files were created successfully
ls -lh output_*.jsonl
rm output_*.jsonl
```erify all files were created successfully
ls -lh output_*.jsonl
rm output_*.jsonl
```

**Monitor:**
- Worker pool logs: "Processing date X" messages
- No requests timing out
- All responses have valid JSONL format

#### Test 5: Frontend Integration
```bash
# Start service locally
go run cmd/alerts-service/main.go

# Open frontend (update API endpoint to localhost:8080)
# Test date selection and data loading
# Verify map renders correctly with compressed data
```

**Validation:**
- [ ] Frontend loads data successfully
- [ ] Map markers render correctly
- [ ] Alert details display properly
- [ ] No console errors about missing fields

### 3.3 Edge Cases

#### Test 6: Mixed Archive/Firestore Query
```bash
# Request dates with both archived and non-archived data
# (This tests parallel execution of different code paths)
curl -H "Accept-Encoding: gzip" \
     "http://localhost:8080/police_alerts?dates=2025-10-15,2025-11-30" \
     --compressed -o mixed_query.jsonl
```

#### Test 7: Empty Result Handling
```bash
# Request a future date with no data
curl -H "Accept-Encoding: gzip" \
     "http://localhost:8080/police_alerts?dates=2026-01-01" \
     --compressed -v

# Expected: 200 OK with empty response body
```

#### Test 8: Malformed Request
```bash
# Test invalid date format
curl "http://localhost:8080/police_alerts?dates=invalid-date" -v

# Expected: 400 Bad Request
```

### 3.4 Production Deployment Testing

#### Cloud Run Deployment
Use the CICD pipeline to deploy to cloud run.

#### Monitor Cloud Run Metrics
- **GCP Console ‚Üí Cloud Run ‚Üí alerts-service ‚Üí Metrics**
  - Request count
  - Request latency (p50, p95, p99)
  - Memory utilization
  - CPU utilization

**Expected Improvements:**
- ‚¨áÔ∏è 70-90% reduction in "Billable instance time" (due to faster response streaming)
- ‚¨áÔ∏è 70-90% reduction in egress bandwidth costs
- ‚û°Ô∏è Memory usage unchanged (already streaming from GCS)

## Phase 4: Monitoring & Rollback Strategy

### 4.1 Key Performance Indicators

| Metric | Baseline | Target | How to Measure |
|--------|----------|--------|----------------|
| Response Size | ~20MB/date | ~2-6MB/date | Cloud Run Metrics ‚Üí Response size |
| Bandwidth Cost | Baseline | -70-90% | GCP Billing ‚Üí Networking egress |
| TTFB (Latency) | Baseline | ¬±5% | Cloud Run Metrics ‚Üí Request latency |
| Error Rate | <1% | <1% | Cloud Run Logs ‚Üí Error count |
| Memory Usage | ~200MB | ¬±10% | Cloud Run Metrics ‚Üí Memory utilization |

### 4.2 Logging Enhancements

**Add to `cmd/alerts-service/main.go`** (in `gzipMiddleware`):
```go
// Optional: Log compression effectiveness
func gzipMiddleware(next http.HandlerFunc) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        if !strings.Contains(r.Header.Get("Accept-Encoding"), "gzip") {
            log.Printf("Client %s does not accept gzip", r.RemoteAddr)
            next(w, r)
            return
        }
        // ... rest of implementation
    }
}
```

### 4.3 Alert Configuration

**Set up Cloud Monitoring alerts:**
1. **High Error Rate**: Alert if error rate >2% for 5 minutes
2. **High Latency**: Alert if p95 latency >10s
3. **Memory Pressure**: Alert if memory usage >80% of limit

### 4.4 Rollback Procedure

**If issues occur after deployment:**

#### Rollback GZIP Compression (Phase 1)
```go
// In cmd/alerts-service/main.go, change:
http.HandleFunc("/police_alerts", corsMiddleware(s.rateLimitMiddleware(gzipMiddleware(s.alertsHandler))))

// To:
http.HandleFunc("/police_alerts", corsMiddleware(s.rateLimitMiddleware(s.alertsHandler)))

// Redeploy
```

**Rollback Time:** ~5 minutes  
**Risk:** None - simply reverts to uncompressed responses

#### Rollback Firestore Streaming (Phase 2)
```go
// In cmd/alerts-service/main.go, change:
iter := s.firestoreClient.GetPoliceAlertsIterator(ctx, startOfDay, endOfDay)
// ... iterator code ...

// Back to:
alerts, firestoreErr := s.firestoreClient.GetPoliceAlertsByDateRange(ctx, startOfDay, endOfDay)
// ... original code ...

// Redeploy
```

**Rollback Time:** ~5 minutes  
**Risk:** Low - only affects non-archived queries

**Note:** Both changes are completely independent and can be rolled back separately.

---

## Implementation Roadmap

### Priority Matrix
**Rollback Time:** ~5 minutes  
**Risk:** Low - only affects non-archived queries

#### Fast Rollback (Recommended)
Use Cloud Run Revisions to immediately revert traffic to the previous stable revision via the GCP Console or CLI. This is faster than redeploying code.

**Note:** Both changes are completely independent and can be rolled back separately.on) | ‚¨ú Not Started |
| **Phase 2: Firestore** | üü° **HIGH** | Medium (4-6 hours) | Medium (prevents OOM) | ‚¨ú Not Started |

### Implementation Order

**Step 1: GZIP Compression (Phase 1)** - 2-3 hours
- [ ] Add `compress/gzip` import
- [ ] Implement `gzipResponseWriter` struct
- [ ] Implement `gzipMiddleware` function
- [ ] Apply middleware to `/police_alerts` route
- [ ] Test locally with curl
- [ ] Deploy to Cloud Run
- [ ] Monitor for 24-48 hours

**Step 2: Firestore Optimization (Phase 2)** - 4-6 hours
- [ ] Add `google.golang.org/api/iterator` import to both services
- [ ] Update `models/alert.go` with `json:"-"` tags
- [ ] Create `GetPoliceAlertsIterator` method with `.Select()` in `police_alerts.go`
- [ ] Update alerts-service handler to use iterator
- [ ] Update archive-service to use iterator
- [ ] Test with non-archived dates
- [ ] Deploy both services to Cloud Run
- [ ] Monitor memory usage

**Step 3: Verification** - 1-2 hours
- [ ] Run all test cases from Phase 3
- [ ] Verify frontend functionality
- [ ] Check Cloud Run metrics
- [ ] Document performance improvements

### Success Criteria

**Phase 1 (GZIP) Success Indicators:**
- [ ] Response headers include `Content-Encoding: gzip` when requested
- [ ] Response size reduced by 70-90%
**Phase 2 (Firestore) Success Indicators:**
- [ ] Memory usage stable during large queries
- [ ] No OOM errors in logs
- [ ] All required fields present in JSON output
- [ ] Query performance maintained or improved

### Risk Mitigation

**Low Risk (Phase 1 - GZIP):**
- Graceful fallback for non-supporting clients
- Independent rollback capability
- No data structure changes

**Medium Risk (Phase 2 - Firestore):**
- Test thoroughly with non-archived dates
- Monitor memory metrics closely
- Keep rollback plan ready

**Important Note on Archives:**
- Existing GCS archives will remain unchanged
- Frontend must continue to handle both formats

---

## Appendix: Technical Reference

### Field Mapping Reference

**Frontend Required Fields** (from `dataAnalysis/public/app.js`):
- `UUID`, `Type`, `Subtype`, `Street`, `City`, `Country`
- `LocationGeo`, `Reliability`, `Confidence`, `ReportRating`
- `PublishTime`, `ExpireTime`, `ScrapeTime`, `ActiveMillis`
- `LastVerificationMillis`, `NThumbsUpLast`

**Firestore Field Names** (snake_case):
- `uuid`, `id`, `type`, `subtype`, `street`, `city`, `country`
- `location_geo`, `reliability`, `confidence`, `report_rating`
- `publish_time`, `expire_time`, `scrape_time`, `active_millis`
- `last_verification_millis`, `n_thumbs_up_last`

### Current Architecture

**Data Flow:**
```
Scraper ‚Üí Firestore (full objects with raw data)
                ‚Üì
        Archive Service ‚Üí GCS (JSONL files)
                ‚Üì
        Alerts Service ‚Üí Client (JSONL stream)
```

**Query Patterns:**
- ~95% of queries hit GCS archives (efficient streaming)
- ~5% query Firestore directly (non-archived dates)

**Optimization Impact:**
- **GZIP:** Affects 100% of queries (high ROI)
- **Firestore Streaming:** Affects ~5% of queries (defensive programming)
